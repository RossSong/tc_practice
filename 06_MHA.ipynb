{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention & Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PyTorch version:[1.7.0].\nThis notebook use [cuda:0].\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"This notebook use [%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model\n",
    "\n",
    "$$\n",
    "\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([1, 60, 512]), torch.Size([1, 60, 60]))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        dk = key.size()[-1]\n",
    "        scores = query.matmul(key.transpose(-2, -1)) / np.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)  \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        out = attention.matmul(value)\n",
    "        return out, attention\n",
    "\n",
    "y = torch.rand(1, 60, 512)\n",
    "out = ScaledDotProductAttention()(y, y, y)\n",
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 45, 512]) torch.Size([1, 8, 45, 60])\n"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, features, num_heads, bias=True):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert features % num_heads == 0, f'\"features\"(features) should be divisible by \"head_num\"(num_heads)'\n",
    "        \n",
    "        self.features = features\n",
    "        self.num_heads = num_heads\n",
    "        self.bias = bias\n",
    "        self.depth = features // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(features, features, bias=bias)\n",
    "        self.wk = nn.Linear(features, features, bias=bias)\n",
    "        self.wv = nn.Linear(features, features, bias=bias)\n",
    "\n",
    "        self.fc = nn.Linear(features, features, bias=bias)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # batch_sie, num_heads, seq_len, depth\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute([0, 2, 1, 3])\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = ScaledDotProductAttention()(q, k, v, mask)\n",
    "        # print(scaled_attention.shape, attention_weights.shape)\n",
    "\n",
    "        scaled_attention = scaled_attention.permute([0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.features)\n",
    "\n",
    "        out = self.fc(concat_attention)\n",
    "\n",
    "        return out, attention_weights\n",
    "\n",
    "temp_mha = MultiHeadAttention(features=512, num_heads=8)\n",
    "out, attn = temp_mha(q=torch.rand(1, 45, 512), k=y, v=y, mask=None)\n",
    "print(out.shape, attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([64, 50, 512])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, features, fffeatures):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        layer_list = [\n",
    "            nn.Linear(features, fffeatures),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(fffeatures, features)\n",
    "        ]\n",
    "\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "sample_ffn = FeedForwardNetwork(512, 2048)\n",
    "sample_ffn(torch.rand(64, 50, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([64, 43, 512])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, features, num_heads, fffeatures, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(features, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(features, fffeatures)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(features)\n",
    "        self.layernorm2 = nn.LayerNorm(features)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    torch.rand(64, 43, 512), None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([64, 50, 512])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, features, num_heads, fffeatures, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(features, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(features, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(features, fffeatures)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(features)\n",
    "        self.layernorm2 = nn.LayerNorm(features)\n",
    "        self.layernorm3 = nn.LayerNorm(features)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        \n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        # print(enc_output.shape)\n",
    "        attn, attn_weights_block = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn = self.dropout1(attn)\n",
    "        out = self.layernorm1(attn + x)\n",
    "        \n",
    "        attn, attn_weights_block = self.mha2(out, enc_output, enc_output, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn = self.dropout2(attn2)\n",
    "        out = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    torch.rand(64, 50, 512), None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1604325676867",
   "display_name": "Python 3.7.9 64-bit ('tc': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}